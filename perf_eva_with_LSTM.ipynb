{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ilf3luxuBB3u",
    "outputId": "58072749-1c50-474c-e6ad-bb77d098cba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data len 51\n",
      "train data len 30\n",
      "validation data len 6\n",
      "specific_validation_data_list len 5\n",
      "total data len 51\n",
      "train data len 46\n",
      "validation data len 5\n",
      "specific_validation_data_list len 5\n",
      "total data len 51\n",
      "train data len 46\n",
      "validation data len 7\n",
      "specific_validation_data_list len 5\n"
     ]
    }
   ],
   "source": [
    "from keras.backend import in_test_phase\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "from keras import backend as K\n",
    "from scipy.spatial import distance\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "import ast\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "# The requirements to use the cuDNN implementation are:\n",
    "\n",
    "# activation == tanh\n",
    "# recurrent_activation == sigmoid\n",
    "# recurrent_dropout == 0\n",
    "# unroll is False\n",
    "# use_bias is True\n",
    "# Inputs, if use masking, are strictly right-padded.\n",
    "# Eager execution is enabled in the outermost context.\n",
    "\n",
    "\n",
    "def get_cross_train_rule_data_and_validation_data(dir):\n",
    "    total_data_list= []\n",
    " \n",
    "    session_list= []\n",
    "    with open(dir, 'r+', newline='') as csvfile:    \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        row_count= 0\n",
    "        for row in reader:\n",
    "            row_list= []\n",
    "            for col in row:\n",
    "                if not col:\n",
    "                    break\n",
    "               \n",
    "                if ast.literal_eval(col)== True:\n",
    "                    data_list= ast.literal_eval(col) \n",
    "                    if not data_list in total_data_list:\n",
    "                        row_list.append(data_list)\n",
    "                else:\n",
    "                    data_int= ast.literal_eval(col) \n",
    "                    if not data_int in total_data_list:\n",
    "                        row_list.append(data_int)\n",
    "            \n",
    "            total_data_list.append(row_list)\n",
    "\n",
    "            row_count+= 1\n",
    "            if row_count> 16: # Select rows\n",
    "                break\n",
    "        \n",
    "\n",
    "        num_of_session= 5    # Number of cross validation session\n",
    "    \n",
    "        for s in range(num_of_session):\n",
    "            start_val_data= int(len(total_data_list)/ num_of_session* s)\n",
    "            num_of_val_data= int(row_count* 1/ 3)\n",
    "            train_rule_data_list= []\n",
    "            validation_data_list= []\n",
    "            new_data=  start_val_data+ num_of_val_data\n",
    "            for row_index in range(len(total_data_list)):\n",
    "                val_data_count= 0\n",
    "                if row_index in range(start_val_data, start_val_data+ num_of_val_data):       \n",
    "                    validation_data_list.append(total_data_list[row_index])\n",
    "                else:\n",
    "                    train_rule_data_list.append(total_data_list[row_index])\n",
    "            session_list.append([train_rule_data_list, validation_data_list])\n",
    "\n",
    "    return session_list\n",
    "\n",
    "def get_rand_train_rule_data_and_validation_data(dir):\n",
    "    total_data_list= []\n",
    " \n",
    "    session_list= []\n",
    "    with open(dir, 'r+', newline='') as csvfile:    \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        row_count= 0\n",
    "        for row in reader:\n",
    "            row_list= []\n",
    "            for col in row:\n",
    "                if not col:\n",
    "                    break\n",
    "\n",
    "                if ast.literal_eval(col)== True:\n",
    "                    data_list= ast.literal_eval(col) \n",
    "                    if not data_list in total_data_list:\n",
    "                        row_list.append(data_list)\n",
    "                else:\n",
    "                    data_int= ast.literal_eval(col) \n",
    "                    if not data_int in total_data_list:\n",
    "                        row_list.append(data_int)\n",
    "            \n",
    "            total_data_list.append(row_list)\n",
    "\n",
    "            row_count+= 1\n",
    "            if row_count> 16: # Select rows\n",
    "                break\n",
    "        \n",
    "        num_of_train_data= int(row_count* 2/ 3)    \n",
    "        num_of_val_data= int(len(total_data_list)- num_of_train_data)\n",
    "        session_num= 2    # cross validation\n",
    "\n",
    "        \n",
    "        rand_sel_record= []\n",
    "        \n",
    "        for session in range(session_num):\n",
    "            total_data_list_copy= total_data_list.copy()\n",
    "            validation_data_list= []\n",
    "            for num in range(num_of_val_data):\n",
    "                rand_sel= random.choice(total_data_list_copy)\n",
    "                while rand_sel in rand_sel_record:\n",
    "                    rand_sel= random.choice(total_data_list_copy)\n",
    "                total_data_list_copy.remove(rand_sel)\n",
    "                validation_data_list.append(rand_sel)\n",
    "                rand_sel_record.append(rand_sel)\n",
    "            train_rule_data_list= total_data_list_copy\n",
    "            session_list.append([train_rule_data_list, validation_data_list])\n",
    "    \n",
    "    return session_list\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_target_train_rule_data_and_validation_data_test_data(dir):\n",
    "    total_data_list= []\n",
    "    target_row= list(range(19, 24))  # 19- 24\n",
    "    target_train_data_index_list= [25]\n",
    "    target_val_data_for_histroy_record= [1, 3, 5, 7, 9, 10, 12]\n",
    "    session_list= []\n",
    "    specific_validation_data_list= []\n",
    "    train_data_list= []\n",
    "    with open(dir, 'r+', newline='' , encoding='utf-8-sig') as csvfile:    \n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        row_count= 0\n",
    "        \n",
    "        for row in reader:\n",
    "          \n",
    "            row_list= []\n",
    "            \n",
    "            if len(row[2])== 0:\n",
    "                continue\n",
    "            for col in row:\n",
    "                if not col:\n",
    "                    break\n",
    "\n",
    "                if ast.literal_eval(col)== True:\n",
    "                    data_list= ast.literal_eval(col) \n",
    "                    if not data_list in total_data_list:\n",
    "                        row_list.append(data_list)\n",
    "                else:\n",
    "                    data_int= ast.literal_eval(col) \n",
    "                    if not data_int in total_data_list:\n",
    "                        row_list.append(data_int)\n",
    "            \n",
    "            total_data_list.append(row_list) \n",
    "            if row_count in target_row:\n",
    "                specific_validation_data_list.append(row_list)\n",
    "            else:\n",
    "                train_data_list.append(row_list)\n",
    "\n",
    "            row_count+= 1\n",
    "        \n",
    "        \n",
    "        num_of_train_data= 30\n",
    "        num_of_val_data_rate= 0.2\n",
    "        num_of_val_data= int(num_of_train_data* num_of_val_data_rate)\n",
    "        num_of_session= 3    # cross validation\n",
    "       \n",
    "       \n",
    "        \n",
    "        for session in range(num_of_session):\n",
    "            validation_data_list= []\n",
    "            train_data_list_copy= train_data_list.copy()\n",
    "            number_of_val_count= 0\n",
    "            \n",
    "           \n",
    "            if session== 0:\n",
    "                while number_of_val_count< num_of_val_data:\n",
    "                    rand_sel= random.choice(train_data_list_copy)\n",
    "                    \n",
    "                    validation_data_list.append(rand_sel)\n",
    "                    number_of_val_count+= 1\n",
    "                train_data_list_copy= train_data_list_copy[: num_of_train_data]\n",
    "                #while len(train_data_list_copy)> num_of_train_data:\n",
    "                #    rand_sel= random.choice(train_data_list_copy)\n",
    "                #    train_data_index= train_data_list.index(rand_sel)\n",
    "                #    if train_data_index in target_train_data_index_list:\n",
    "                #        continue\n",
    "                #    train_data_list_copy.remove(rand_sel)\n",
    "                    \n",
    "                target_val_data_list= validation_data_list\n",
    "            if session== 1:\n",
    "                target_val_data_list= specific_validation_data_list\n",
    "            \n",
    "\n",
    "            if session== 2:\n",
    "                for i in target_val_data_for_histroy_record:\n",
    "                    target_sel= train_data_list_copy[i]\n",
    "                    \n",
    "                   # specific_validation_data_list.remove(target_sel)\n",
    "                    number_of_val_count+= 1\n",
    "                    validation_data_list.append(target_sel)\n",
    "                target_val_data_list= validation_data_list\n",
    "\n",
    "          \n",
    "                \n",
    "            \n",
    "            session_list.append([train_data_list_copy, target_val_data_list])\n",
    "\n",
    "                \n",
    "            print ('total data len', len(total_data_list))\n",
    "            print ('train data len', len(train_data_list_copy))\n",
    "            print ('validation data len', len(target_val_data_list))\n",
    "            print ('specific_validation_data_list len', len(specific_validation_data_list))\n",
    "  \n",
    "    \n",
    "    return session_list\n",
    "\n",
    "def save_train_and_val_data(dir, data):\n",
    "    with open(dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def save_train_history_data(dir, data_list):\n",
    "    with open(dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "\n",
    "def get_train_history_data(dir):\n",
    "    with open(dir, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        data_list= [i for i in reader]\n",
    "    return data_list\n",
    "\n",
    "def get_train_and_val_data(dir):\n",
    "    with open(dir, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        content= []\n",
    "        for sub_content in reader:\n",
    "            sub_content_list= []\n",
    "            for s_s_content in sub_content:\n",
    "                s_s_c= ast.literal_eval(s_s_content)\n",
    "                sub_content_list.append(s_s_c)\n",
    "            content.append(sub_content_list)\n",
    "      \n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_layer_output_value(model, input_data_list_arr):\n",
    "    inp = model.input\n",
    "    outputs = [layer.output for layer in model.layers]   \n",
    "    functors = [K.function([inp], [out]) for out in outputs] \n",
    "    layer_outs = [func([input_data_list_arr]) for func in functors]\n",
    "    print ('layer_outs shape', np.array(layer_outs).shape)\n",
    "   \n",
    "\n",
    "  \n",
    "def sub_cross_train_rule_data(train_rule_data):\n",
    "    session_num= 5\n",
    "    train_rule_data_len= len(train_rule_data)\n",
    "    session_list= []\n",
    "\n",
    "    for s in range(session_num):\n",
    "        start_split_data_index= int(train_rule_data_len/ session_num* s)\n",
    "        end_split_data_index= start_split_data_index+ train_rule_data_len-2\n",
    "        if end_split_data_index> train_rule_data_len:\n",
    "            end_split_data_index- train_rule_data_len\n",
    "        if start_split_data_index< end_split_data_index:\n",
    "            session_list.append(train_rule_data[start_split_data_index:end_split_data_index])\n",
    "        else:\n",
    "            session_list.append(train_rule_data[0:end_split_data_index]+ train_rule_data[start_split_data_index:-1])\n",
    "\n",
    "    return session_list    \n",
    "    \n",
    "\n",
    "def sort_to_combination(sort_list):\n",
    "    combination_list= []\n",
    "    for sub_component_index in range(len(sort_list)):\n",
    "        if sub_component_index== len(sort_list)- 1:\n",
    "            break\n",
    "        sub_component= [sort_list[sub_component_index], sort_list[sub_component_index+ 1]]\n",
    "        combination_list.append(sub_component)\n",
    "    return combination_list\n",
    "\n",
    "def data_dist_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    data_dist_list= []\n",
    "    all_segments= expected_data_sort+ outside_segment_data_sort\n",
    "    outside_segment_info= len(all_segments)+ 1\n",
    "    component_position_in_train_rule_data_list= []\n",
    "    component_position_in_expected_data_list= []\n",
    "    component_position_in_outside_data_list= []\n",
    "    if component_data in expected_data_sort:\n",
    "        for segment_index in expected_data_sort:\n",
    "      #  if not segment_index in train_rule_data_sort or not component_data in train_rule_data_sort: # The segments not in expected data. The dist== length of expected data + 1\n",
    "      #      data_dist=  len(all_segments)+ 1\n",
    "      #      data_dist_list.append(data_dist)\n",
    "      #      continue\n",
    "    \n",
    "            component_data_index= train_rule_data_sort.index(component_data) # Component position\n",
    "            expected_data_index= expected_data_sort.index(segment_index) # Each segments position\n",
    "            data_dist= abs(expected_data_index- component_data_index) # The distance of component data position to expected data position. If the component data not in expected data dist info == 0.\n",
    "            data_dist_list.append(data_dist) \n",
    "            component_position_in_train_rule_data_list.append(component_data_index)\n",
    "            component_position_in_expected_data_list.append(expected_data_index)\n",
    "        for outside_segment_index in outside_segment_data_sort:\n",
    "            data_dist_list.append(outside_segment_info)\n",
    "        \n",
    "        dist_val= distance.minkowski(component_position_in_train_rule_data_list, component_position_in_expected_data_list, len(component_position_in_train_rule_data_list))\n",
    "        dist_coding_list= []\n",
    "        dist_val= list(str(dist_val))\n",
    "        dist_val.remove('.')\n",
    "        for dist in dist_val:\n",
    "            dist_coding_list.append(int(dist))\n",
    "        while len(dist_coding_list)< 16:\n",
    "            dist_coding_list.append(0)\n",
    "        while len(dist_coding_list)> 16:\n",
    "            dist_coding_list.pop(-1)\n",
    "        dist_coding_list= list(np.array(dist_coding_list)+ len(expected_data_sort)*2)\n",
    "\n",
    "    else:\n",
    "        for segment_index in expected_data_sort:\n",
    "            data_dist_list.append(outside_segment_info)\n",
    "\n",
    "        for outside_segment_index in outside_segment_data_sort:\n",
    "            component_data_index= train_rule_data_sort.index(component_data)\n",
    "            outside_data_index= outside_segment_data_sort.index(outside_segment_index)\n",
    "\n",
    "            component_position_in_train_rule_data_list.append(component_data_index)\n",
    "            component_position_in_outside_data_list.append(outside_data_index)\n",
    "\n",
    "            data_dist= abs(outside_data_index- component_data_index)\n",
    "            data_dist_list.append(data_dist) \n",
    "        \n",
    "        dist_val= distance.minkowski(component_position_in_train_rule_data_list, component_position_in_outside_data_list, len(component_position_in_train_rule_data_list))\n",
    "        dist_coding_list= []\n",
    "        dist_val= list(str(dist_val))\n",
    "        dist_val.remove('.')\n",
    "        for dist in dist_val:\n",
    "            dist_coding_list.append(int(dist))\n",
    "        while len(dist_coding_list)< 16:\n",
    "            dist_coding_list.append(0)\n",
    "        while len(dist_coding_list)> 16:\n",
    "            dist_coding_list.pop(-1)\n",
    "        dist_coding_list= list(np.array(dist_coding_list)+ len(expected_data_sort)*2)\n",
    "\n",
    "    return dist_coding_list\n",
    "\n",
    "\n",
    "def data_conn_in_different_level_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    data_conn_info_list= []\n",
    "    all_segments= expected_data_sort+ outside_segment_data_sort\n",
    "    component_data_index= train_rule_data_sort.index(component_data)\n",
    "    conn= len(all_segments)+ 7\n",
    "    not_conn= len(all_segments)+ 8\n",
    "    \n",
    "\n",
    "   \n",
    "    still_conn= True\n",
    "        \n",
    "    for next_conn_in_order in range(1, len(expected_data_sort)):\n",
    "        conn_level= not_conn+ next_conn_in_order\n",
    "        data_conn_info_list.append(conn_level)        \n",
    "        if component_data in expected_data_sort:\n",
    "            component_data_categ= expected_data_sort.index(component_data)\n",
    "            component_data_in_train_data_categ= train_rule_data_sort.index(component_data)     \n",
    "            \n",
    "\n",
    "            conn_info= None\n",
    "            \n",
    "            if component_data_categ== len(expected_data_sort):\n",
    "                conn_info= not_conn\n",
    "                still_conn= False\n",
    "            \n",
    "                \n",
    "            else:    \n",
    "                conn_categ_train_data=  component_data_in_train_data_categ+ next_conn_in_order     \n",
    "                conn_categ= component_data_categ+ next_conn_in_order\n",
    "\n",
    "                if conn_categ_train_data>= len(train_rule_data_sort):\n",
    "                   \n",
    "                    conn_info= not_conn\n",
    "                \n",
    "              \n",
    "                if conn_categ>= len(train_rule_data_sort):\n",
    "                    conn_info= not_conn               \n",
    "                \n",
    "              \n",
    "                if conn_categ_train_data< len(train_rule_data_sort) and conn_categ< len(train_rule_data_sort):\n",
    "                    if train_rule_data_sort[conn_categ_train_data] ==  expected_data_sort[conn_categ]:\n",
    "                        if still_conn:\n",
    "                            conn_info= conn\n",
    "                        else:\n",
    "                            conn_info= not_conn\n",
    "                    else:\n",
    "                        conn_info= not_conn\n",
    "               \n",
    "\n",
    "        else: \n",
    "            component_data_categ= outside_segment_data_sort.index(component_data)\n",
    "            outside_component_data_categ= len(expected_data_sort)+ component_data_categ\n",
    "              \n",
    "            conn_info= not_conn\n",
    "\n",
    "        data_conn_info_list.append(conn_info)\n",
    "    \n",
    "    return data_conn_info_list\n",
    "\n",
    "\n",
    "def data_conn_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    data_conn_info_list= []\n",
    "    all_segments= expected_data_sort+ outside_segment_data_sort\n",
    "    component_data_index= train_rule_data_sort.index(component_data)\n",
    "    conn= len(all_segments)+ 1\n",
    "    not_conn= len(all_segments)+ 2\n",
    "    \n",
    "    \n",
    "    for segment in train_rule_data_sort: \n",
    "        conn_info= None\n",
    "        if segment in expected_data_sort:\n",
    "            segment_categ= expected_data_sort.index(segment)\n",
    "            data_conn_info_list.append(segment_categ)\n",
    "           \n",
    "            if segment_categ== len(expected_data_sort):\n",
    "                conn_info= not_conn\n",
    "               \n",
    "                \n",
    "            else:\n",
    "                conn_categ_train_data= segment+ 1\n",
    "                conn_categ= segment_categ+ 1\n",
    "                \n",
    "                if conn_categ_train_data== len(train_rule_data_sort):\n",
    "                    conn_info= not_conn\n",
    "                if conn_categ== len(train_rule_data_sort):\n",
    "                    conn_info= not_conn\n",
    "                    \n",
    "                if conn_categ_train_data!= len(train_rule_data_sort) and conn_categ!= len(train_rule_data_sort):\n",
    "                    if train_rule_data_sort[conn_categ_train_data] ==  expected_data_sort[conn_categ]:\n",
    "                        conn_info= conn\n",
    "                    else:\n",
    "                        conn_info= not_conn\n",
    "        \n",
    "        else: \n",
    "            segment_categ= outside_segment_data_sort.index(segment)\n",
    "            outside_component_data_categ= len(expected_data_sort)+ segment_categ\n",
    "            data_conn_info_list.append(outside_component_data_categ)          \n",
    "            conn_info= not_conn\n",
    "      \n",
    "\n",
    "        data_conn_info_list.append(conn_info)\n",
    "            \n",
    "    \n",
    "    if component_data in expected_data_sort:\n",
    "        conn_info= None\n",
    "        component_data_categ= expected_data_sort.index(component_data)\n",
    "        component_data_in_train_data_categ= train_rule_data_sort.index(component_data)\n",
    "        data_conn_info_list.append(component_data_categ)\n",
    "        if component_data_categ== len(expected_data_sort):\n",
    "            conn_info= not_conn\n",
    "            \n",
    "                \n",
    "        else:    \n",
    "            conn_categ_train_data=  component_data_in_train_data_categ+ 1       \n",
    "            conn_categ= component_data_categ+ 1\n",
    "\n",
    "            if conn_categ_train_data== len(train_rule_data_sort):\n",
    "                conn_info= not_conn\n",
    "                \n",
    "              \n",
    "            if conn_categ== len(train_rule_data_sort):\n",
    "                conn_info= not_conn               \n",
    "                \n",
    "              \n",
    "            if conn_categ_train_data!= len(train_rule_data_sort) and conn_categ!= len(train_rule_data_sort):\n",
    "                if train_rule_data_sort[conn_categ_train_data] ==  expected_data_sort[conn_categ]:\n",
    "                    conn_info= conn\n",
    "                else:\n",
    "                    conn_info= not_conn\n",
    "               \n",
    "            \n",
    "\n",
    "    else: \n",
    "        component_data_categ= outside_segment_data_sort.index(component_data)\n",
    "        outside_component_data_categ= len(expected_data_sort)+ component_data_categ\n",
    "        data_conn_info_list.append(outside_component_data_categ)          \n",
    "        conn_info= not_conn\n",
    "\n",
    "    data_conn_info_list.append(conn_info)\n",
    "    \n",
    "    return data_conn_info_list\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    all_segments_len= len(expected_data_sort+ outside_segment_data_sort)\n",
    "    r_posi= all_segments_len+ 3\n",
    "    not_r_posi= all_segments_len+ 4\n",
    "    all_segments_data_posi_info_list= []\n",
    "    for i, segment in enumerate(train_rule_data_sort):\n",
    "        posi_cate= i+ len(expected_data_sort)\n",
    "        posi_info= None\n",
    "        if not segment in expected_data_sort:\n",
    "            \n",
    "            posi_info= not_r_posi\n",
    "            all_segments_data_posi_info_list.append(posi_cate)\n",
    "            all_segments_data_posi_info_list.append(posi_info)\n",
    "            continue \n",
    "           \n",
    "        component_data_index= train_rule_data_sort.index(segment)\n",
    "        expected_data_index= expected_data_sort.index(segment)\n",
    "        if component_data_index== expected_data_index:\n",
    "            posi_info= r_posi\n",
    "        else:\n",
    "            posi_info= not_r_posi\n",
    "        all_segments_data_posi_info_list.append(posi_cate)\n",
    "        all_segments_data_posi_info_list.append(posi_info)\n",
    "\n",
    "    return all_segments_data_posi_info_list\n",
    "\n",
    "\n",
    "\n",
    "def data_posi_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    all_segment_len= len(expected_data_sort+ outside_segment_data_sort)\n",
    "    all_segment= expected_data_sort+ outside_segment_data_sort\n",
    "   \n",
    "    r_posi= all_segment_len+ 5\n",
    "    not_r_posi= all_segment_len+ 6\n",
    "    posi_info= None\n",
    "\n",
    "    if not component_data in expected_data_sort:\n",
    "        \n",
    "        posi_info= not_r_posi\n",
    "    else:\n",
    "        component_data_index= train_rule_data_sort.index(component_data)\n",
    "        expected_data_index= expected_data_sort.index(component_data)\n",
    "        if component_data_index== expected_data_index:\n",
    "            posi_info= r_posi\n",
    "        else:\n",
    "            posi_info= not_r_posi\n",
    "\n",
    "    return posi_info\n",
    "    \n",
    "def get_activity_as_represntation(data_list):\n",
    "    number_of_units= len(data_list)\n",
    "    data_list_np= np.expand_dims(np.array(data_list), 0)\n",
    "    output= tf.keras.layers.Dense(units = number_of_units, activation='sigmoid', use_bias=True)(data_list_np)\n",
    "    output= output.numpy().tolist()\n",
    "    return output\n",
    "\n",
    "def in_right_dataset_info(component_data, train_rule_data_sort, expected_data_sort, outside_segment_data_sort):\n",
    "    all_segment_len= len(expected_data_sort+ outside_segment_data_sort)\n",
    "   \n",
    "    in_right_d= all_segment_len+ 7\n",
    "    not_in_right_d= all_segment_len+ 8\n",
    "    in_right_info= not_in_right_d\n",
    "\n",
    "    if not component_data in expected_data_sort:\n",
    "        in_right_info= not_in_right_d\n",
    "    else:\n",
    "       \n",
    "        in_right_info= in_right_d\n",
    "    \n",
    "    return in_right_info\n",
    "\n",
    "\n",
    "def get_input_data(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list):\n",
    "    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]\n",
    "    input_data_index_list= []\n",
    "    validation_input_data_list= []\n",
    "    validation_data_input_index_list= []\n",
    "\n",
    "  \n",
    "    for train_rule_data in train_rule_data_list:            \n",
    "        train_rule_data_sort= train_rule_data[0]\n",
    "        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)\n",
    "        for train_rule_data_comb in train_rule_data_comb_list:\n",
    "            first_component= train_rule_data_comb[0]\n",
    "            second_component= train_rule_data_comb[1]\n",
    "         #   first_comp_data_dist_info_train_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "         #   second_comp_data_dist_info_train_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "         #   data_conn_in_different_level_list= data_conn_in_different_level_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            first_comp_data_conn_info_train_list= data_conn_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "         #   second_comp_data_conn_info_train_list= data_conn_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            \n",
    "            first_comp_data_posi_info= data_posi_info(first_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            second_comp_data_posi_info= data_posi_info(second_component, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            \n",
    "       \n",
    "            \n",
    "            input_data= first_comp_data_conn_info_train_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_info]+ [second_comp_data_posi_info]\n",
    "           \n",
    "\n",
    "          #  activity_as_representation= get_activity_as_represntation(input_data)\n",
    "            input_data_prepro= on_preprocessing_layer(input_data, len(input_data))\n",
    "           \n",
    "            input_data= input_data_prepro          \n",
    "            input_data_list.append(input_data)\n",
    "      \n",
    "\n",
    "            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])\n",
    "\n",
    "    for validation_data in validation_data_list: \n",
    "        validation_data_sort= validation_data[0]\n",
    "        validation_data_comb_list= sort_to_combination(validation_data_sort)\n",
    "        for validation_data_comb in validation_data_comb_list:\n",
    "            val_data_first_component= validation_data_comb[0]\n",
    "            val_data_second_component= validation_data_comb[1]\n",
    "            \n",
    "          #  first_comp_data_dist_info_val_list= data_dist_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "          #  second_comp_data_dist_info_val_list= data_dist_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "\n",
    "         #   all_data_conn_info_list= all_data_conn_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "          #  val_data_conn_in_different_level_list= data_conn_in_different_level_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            first_comp_data_conn_info_val_list= data_conn_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "          #  second_comp_data_conn_info_val_list= data_conn_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            all_segments_data_posi_info_list= all_segments_data_posi_info(train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "\n",
    "            first_comp_data_posi_info= data_posi_info(val_data_first_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            second_comp_data_posi_info= data_posi_info(val_data_second_component, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "\n",
    "           \n",
    "            val_input_data= first_comp_data_conn_info_val_list+ all_segments_data_posi_info_list+ [first_comp_data_posi_info]+ [second_comp_data_posi_info]\n",
    "           \n",
    "\n",
    "         #   activity_as_representation= get_activity_as_represntation(val_input_data)  \n",
    "            val_input_data_prepro= on_preprocessing_layer(val_input_data, len(val_input_data))\n",
    "            val_input_data= val_input_data_prepro\n",
    "            validation_input_data_list.append(val_input_data)\n",
    "\n",
    "            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])\n",
    "  \n",
    "    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list\n",
    "\n",
    "def get_input_data_posi_val(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list):\n",
    "    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]\n",
    "    input_data_index_list= []\n",
    "    validation_input_data_list= []\n",
    "    validation_data_input_index_list= []\n",
    "    for train_rule_data in train_rule_data_list:            \n",
    "        train_rule_data_sort= train_rule_data[0]      \n",
    "        for train_rule_data_comb in train_rule_data_sort:  \n",
    "            comp_data_conn_info_list= data_conn_info(train_rule_data_comb, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            comp_data_posi_info= data_posi_info(train_rule_data_comb, train_rule_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "         \n",
    "            input_data= comp_data_conn_info_list\n",
    "            input_data.append(comp_data_posi_info)\n",
    "\n",
    "            input_data_list.append(input_data)\n",
    "      \n",
    "\n",
    "            input_data_index_list.append([train_rule_data_sort, train_rule_data_comb])\n",
    "    for validation_data in validation_data_list:            \n",
    "        validation_data_sort= validation_data[0]       \n",
    "        for val_data_comb in validation_data_sort:\n",
    "            comp_data_conn_info_list= data_conn_info(val_data_comb, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "            comp_data_posi_info= data_posi_info(val_data_comb, validation_data_sort, expected_data_sort, outside_segment_data_sort)\n",
    "\n",
    "            val_input_data= comp_data_conn_info_list\n",
    "            val_input_data.append(comp_data_posi_info)\n",
    "\n",
    "            validation_input_data_list.append(val_input_data)\n",
    "            validation_data_input_index_list.append([validation_data_sort, val_data_comb])\n",
    "  \n",
    "    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list\n",
    "\n",
    "def get_input_data_0(expected_data_sort, train_rule_data_list, validation_data_list):\n",
    "    input_data_list= [] # [ data numbers* component numbers- 1, 2, component numbers of expected data -1]\n",
    "    input_data_index_list= []\n",
    "    validation_input_data_list= []\n",
    "    validation_data_input_index_list= []\n",
    "    for train_rule_data in train_rule_data_list:            \n",
    "        train_rule_data_sort= train_rule_data[0]\n",
    "        train_rule_data_comb_list= sort_to_combination(train_rule_data_sort)\n",
    "        for train_rule_data_comb in train_rule_data_comb_list:\n",
    "            first_component= train_rule_data_comb[0]\n",
    "            second_component= train_rule_data_comb[1]\n",
    "        \n",
    "            first_comp_data_dist_info_list= data_dist_info(first_component, train_rule_data_sort, expected_data_sort)\n",
    "            second_comp_data_dist_info_list= data_dist_info(second_component, train_rule_data_sort, expected_data_sort)\n",
    "            first_comp_data_posi_info= data_posi_info(first_component, train_rule_data_sort, expected_data_sort)\n",
    "            second_comp_data_posi_info= data_posi_info(second_component, train_rule_data_sort, expected_data_sort)\n",
    "     \n",
    "  \n",
    "            input_data= train_rule_data_sort+ train_rule_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list\n",
    "            input_data.append(first_comp_data_posi_info)\n",
    "            input_data.append(second_comp_data_posi_info)\n",
    "            input_data_list.append(input_data)\n",
    "      \n",
    "\n",
    "            input_data_index_list.append([train_rule_data_sort, [first_component, second_component]])\n",
    "    for validation_data in validation_data_list:            \n",
    "        validation_data_sort= validation_data[0]\n",
    "        validation_data_comb_list= sort_to_combination(validation_data_sort)\n",
    "        for validation_data_comb in validation_data_comb_list:\n",
    "            val_data_first_component= validation_data_comb[0]\n",
    "            val_data_second_component= validation_data_comb[1]\n",
    "\n",
    "            first_comp_data_dist_info_list= data_dist_info(val_data_first_component, train_rule_data_sort, expected_data_sort)\n",
    "            second_comp_data_dist_info_list= data_dist_info(val_data_second_component, train_rule_data_sort, expected_data_sort)\n",
    "            first_comp_data_posi_info= data_posi_info(val_data_first_component, train_rule_data_sort, expected_data_sort)\n",
    "            second_comp_data_posi_info= data_posi_info(val_data_second_component, train_rule_data_sort, expected_data_sort)\n",
    "\n",
    "    \n",
    "            val_input_data= validation_data_sort+ validation_data_comb+ first_comp_data_dist_info_list+ second_comp_data_dist_info_list\n",
    "            val_input_data.append(first_comp_data_posi_info)\n",
    "            val_input_data.append(second_comp_data_posi_info)\n",
    "        \n",
    "\n",
    "            validation_input_data_list.append(val_input_data)\n",
    "            validation_data_input_index_list.append([validation_data_sort, [val_data_first_component, val_data_second_component]])\n",
    "  \n",
    "    return input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list\n",
    "\n",
    "\n",
    "\n",
    "def get_expected_output_data(input_data_index_list, train_rule_data_list, output_list):\n",
    "    all_output_val_list= []\n",
    "    train_rule_data_index_list= [i[0] for i in train_rule_data_list]\n",
    "    for input_data_index in input_data_index_list:\n",
    "        target_train_rule_data_index= train_rule_data_index_list.index(input_data_index[0])\n",
    "        target_train_rule_data= train_rule_data_list[target_train_rule_data_index]\n",
    "        for component_index in range(len(target_train_rule_data)):\n",
    "            if component_index== 0:\n",
    "                continue\n",
    "          \n",
    "            if component_index% 2== 1 and type(target_train_rule_data[component_index])== list:\n",
    "                if input_data_index[1] == [target_train_rule_data[component_index][0], target_train_rule_data[component_index][2]]:                     \n",
    "                    target_output= target_train_rule_data[component_index+ 1]\n",
    "                    output_index_list= [i[0] for i in output_list]\n",
    "                    target_output_index= output_index_list.index(str(target_output))\n",
    "                    output_val_list= [i[1] for i in output_list]\n",
    "                    output_val_list[target_output_index]= 1\n",
    "        all_output_val_list.append(output_val_list)\n",
    "        \n",
    "    return all_output_val_list\n",
    "        \n",
    "\n",
    "def get_expected_output_data_posi_val(input_data_index_list, train_rule_data_list, output_list):\n",
    "    all_output_val_list= [] \n",
    "    train_rule_data_index_list= [i[0] for i in train_rule_data_list]\n",
    "    for input_data_index in input_data_index_list:\n",
    "        target_train_rule_data_index= train_rule_data_index_list.index(input_data_index[0])\n",
    "        target_train_rule_data= train_rule_data_list[target_train_rule_data_index]\n",
    "        for component_index in range(len(target_train_rule_data)):\n",
    "            if component_index== 0:\n",
    "                continue\n",
    "            if component_index% 2== 1 and type(target_train_rule_data[component_index])== int:\n",
    "                if input_data_index[1] == target_train_rule_data[component_index]:\n",
    "                    target_output= target_train_rule_data[component_index+ 1]\n",
    "          \n",
    "                    output_index_list= [i[0] for i in output_list]\n",
    "                    target_output_index= output_index_list.index(str(target_output))\n",
    "                    output_val_list= [i[1] for i in output_list]\n",
    "                    output_val_list[target_output_index]= 1\n",
    "        all_output_val_list.append(output_val_list)\n",
    "\n",
    "    return all_output_val_list\n",
    "\n",
    "\n",
    "def get_expected_output_data_all_data_level(input_data_index_list, train_rule_data_list, output_list):\n",
    "    all_output_list= []\n",
    "  \n",
    "    train_rule_data_index_list= [i[0] for i in train_rule_data_list]\n",
    "    for input_data_index in range(len(input_data_index_list)):\n",
    "    \n",
    "        output_val_list= list(np.zeros(len(input_data_index_list)))\n",
    "        output_val_list[input_data_index]= 1\n",
    "        all_output_list.append(output_val_list)\n",
    "    return all_output_list\n",
    "\n",
    "def get_current_result(session_list, session_index, expected_data_sort, outside_segment_data_sort, output_list, save_model_dir):\n",
    "    train_rule_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "        \n",
    "    input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list)\n",
    "   \n",
    "    model= load_model(save_model_dir)\n",
    "    sse, mse= init_predict_on_training(model, expected_data_sort, validation_input_data_list, validation_data_input_index_list, validation_data_list, output_list)\n",
    "\n",
    "    return sse\n",
    "\n",
    "def dropout_non_connect(model):\n",
    "    target_layer_id_list= [1, 2, 3, 4, 5, 6]\n",
    "    for layer_id in target_layer_id_list:\n",
    "\n",
    "                target_lstm_layer_weights = model.layers[layer_id].get_weights()[0]\n",
    "                target_lstm_layer_weights_shape= target_lstm_layer_weights.shape\n",
    "                target_lstm_layer_weights_size= target_lstm_layer_weights.size\n",
    "                new_target_lstm_layer_weights= target_lstm_layer_weights.reshape(target_lstm_layer_weights_size)\n",
    "                \n",
    "                for weight in new_target_lstm_layer_weights:\n",
    "                    if weight< 0:\n",
    "                        weight= 0\n",
    "                  \n",
    "                new_target_lstm_layer_weights= new_target_lstm_layer_weights.reshape(target_lstm_layer_weights_shape)\n",
    "                new_param=  model.layers[layer_id].get_weights()\n",
    "                new_param[0]= new_target_lstm_layer_weights           \n",
    "                model.layers[layer_id].set_weights(new_param)\n",
    "                \n",
    "    return model\n",
    "\n",
    "def output_as_feedback_signal(model):\n",
    "   # get_last_layer_output = K.function([model.layers[0].input],\n",
    "   #                               [model.layers[-1].output])\n",
    "   # last_layer_output = get_last_layer_output([])[0]\n",
    "    last_layer_output= model.layers[-1].output\n",
    "    print (last_layer_output.get_shape())\n",
    "    print (last_layer_output)\n",
    "    target_layer_id_list= [1, 2, 3, 4, 5, 6]\n",
    "    for layer_id in target_layer_id_list:\n",
    "\n",
    "                target_lstm_layer_weights = model.layers[layer_id].get_weights()[0]\n",
    "                target_lstm_layer_weights_shape= target_lstm_layer_weights.shape\n",
    "                target_lstm_layer_weights_size= target_lstm_layer_weights.size\n",
    "                new_target_lstm_layer_weights= target_lstm_layer_weights.reshape(target_lstm_layer_weights_size)\n",
    "                \n",
    "                for weight in new_target_lstm_layer_weights:\n",
    "                    if weight< 0:\n",
    "                        weight= 0\n",
    "                  \n",
    "                new_target_lstm_layer_weights= new_target_lstm_layer_weights.reshape(target_lstm_layer_weights_shape)\n",
    "                new_param=  model.layers[layer_id].get_weights()\n",
    "                new_param[0]= new_target_lstm_layer_weights           \n",
    "                model.layers[layer_id].set_weights(new_param)\n",
    "                \n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def init_training(expected_data_sort, outside_segment_data_sort, session_list, output_list, num_of_layers, model, memory_state, carry_state, save_model_dir, checkpoint_dir, save_train_history_dir, train_rule_data_dir):\n",
    "    init_train= True\n",
    "    init_next_layer= False\n",
    "    save_model_dir=  save_model_dir\n",
    "    checkpoint_dir= checkpoint_dir \n",
    "    \n",
    "\n",
    "    seed= 10\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "    state= (memory_state, carry_state)\n",
    "    layer_count= 0\n",
    "    session_index= 0\n",
    "    num_of_layers= num_of_layers\n",
    "    train_times_count= 0\n",
    "    call_back_stop_count= 0\n",
    "    call_back_stop_val= 5\n",
    "    pred_result_list= []\n",
    " \n",
    "    reset_train_history_data= True\n",
    "    if os.path.exists(save_train_history_dir) or reset_train_history_data== False:\n",
    "        result_history_list= get_train_history_data(save_train_history_dir)\n",
    "    if reset_train_history_data:\n",
    "        result_history_list= []\n",
    "     # Get input, output data\n",
    "    train_rule_data_list= session_list[session_index][0]\n",
    "    validation_data_list= session_list[session_index][1]\n",
    "    # train_rule_data_list= sub_cross_train_rule_data(train_rule_data_list)[sub_session_index]\n",
    "        \n",
    "\n",
    "       \n",
    "    input_data_list, input_data_index_list, validation_input_data_list, validation_data_input_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list)\n",
    "\n",
    "    output_value_list= get_expected_output_data(input_data_index_list, train_rule_data_list, output_list)  # Output expeceted data    \n",
    "   \n",
    "\n",
    "\n",
    "  \n",
    "   \n",
    "\n",
    "    if model:\n",
    "        current_result= get_current_result(session_list, session_index, expected_data_sort, outside_segment_data_sort, output_list, save_model_dir)\n",
    "        pred_result_list.append(current_result)\n",
    "\n",
    "    while init_train:\n",
    "        gc.enable()\n",
    "        gc.collect()\n",
    "        print ('previous pred_result_list', pred_result_list)\n",
    "        \n",
    "        print ('length of train data', len(train_rule_data_list))\n",
    "        print ('length of validation data', len(validation_data_list))\n",
    "        \n",
    "   \n",
    "        input_data_list_arr= np.array(input_data_list)  # Input data\n",
    "        output_value_list_arr= np.array(output_value_list)  # Output data\n",
    "\n",
    "\n",
    "     \n",
    "        batch_size, time_steps, features= input_data_list_arr.shape\n",
    "        print ('batch', batch_size, 'time_steps', time_steps, 'feature', features)\n",
    "        \n",
    "        \n",
    "\n",
    "      #  if model:\n",
    "      #      model= dropout_non_connect(model)\n",
    "         \n",
    "\n",
    "        if not model:      \n",
    "            #####  Build model  #####   \n",
    "            input_layer= on_input_layer(input_data_list_arr, batch_size, time_steps, features) # For encoder, decoder\n",
    "\n",
    "            layer= input_layer\n",
    "            init_next_layer= True\n",
    "            while init_next_layer:\n",
    "                if layer_count== num_of_layers:\n",
    "                    break\n",
    "                new_layer, memory_state, carry_state= on_next_layer(layer, layer_count, num_of_layers, state, time_steps, features)\n",
    "                state= (memory_state, carry_state)\n",
    "                layer= new_layer\n",
    "                layer_count+= 1\n",
    "\n",
    "      \n",
    "            output_layer= on_output_layer(input_data_list_arr, output_value_list_arr, layer, time_steps, features)  \n",
    "\n",
    "            model = tf.keras.Model(inputs= input_layer, outputs= output_layer, name= 'reward_predict_model') # Multi\n",
    "            \n",
    "\n",
    "            #  model= build_model()\n",
    "  \n",
    "\n",
    "    \n",
    "        #### Batch  training\n",
    "        tf.keras.backend.clear_session()\n",
    "        history= train_model(input_data_list_arr, output_value_list_arr, model, checkpoint_dir, batch_size)\n",
    "        model_c= tf.keras.models.clone_model(\n",
    "            model, input_tensors=None, clone_function=None\n",
    "        )\n",
    "       \n",
    "\n",
    "        val_accuracy_list= history.history['val_accuracy']\n",
    "        accuracy_list= history.history['accuracy']\n",
    "\n",
    "\n",
    "        get_layer_output_value(model, input_data_list_arr)\n",
    "\n",
    "        init_pred= False\n",
    "        \n",
    "        if train_times_count% 5== 0 or init_pred:\n",
    "            if max(np.array(accuracy_list))>=0.9:\n",
    "                tf.keras.backend.clear_session()\n",
    "                sse, mse= init_predict_on_training(model_c, expected_data_sort, validation_input_data_list, validation_data_input_index_list, validation_data_list, output_list) \n",
    "                print ('sse :', sse)\n",
    "                pred_result_list.append(sse)\n",
    "                best_result= min(pred_result_list)\n",
    "                if len(pred_result_list)> 1 and sse== best_result:\n",
    "                    save_model(model, save_model_dir)\n",
    "                    print ('Save the best result :', sse)\n",
    "\n",
    "                    call_back_stop_count= 0\n",
    "                call_back_stop_count+= 1\n",
    "\n",
    "        if call_back_stop_count== call_back_stop_val:                   \n",
    "            break\n",
    "\n",
    "\n",
    "        if train_times_count% 20== 0:\n",
    "            if not max(np.array(accuracy_list))>=0.9:\n",
    "                save_model(model, save_model_dir)\n",
    "        \n",
    "        if train_times_count% 5== 0:\n",
    "            session_index= 1\n",
    "\n",
    "            train_rule_data_list_for_hist= session_list[session_index][0]\n",
    "            validation_data_list_for_hist= session_list[session_index][1]\n",
    "            predict_data_list_for_hist= validation_data_list\n",
    "\n",
    "            input_data_list_for_hist, input_data_index_list_for_hist, val_input_data_list_for_hist, val_data_input_index_list_for_hist= get_input_data(expected_data_sort, outside_segment_data_sort, train_rule_data_list_for_hist, validation_data_list_for_hist)\n",
    "\n",
    "            print (len(train_rule_data_list_for_hist), train_rule_data_list_for_hist)\n",
    "            print (len(predict_data_list_for_hist), predict_data_list_for_hist)\n",
    "\n",
    "            sse, mse= init_predict_on_training(model_c, expected_data_sort, val_input_data_list_for_hist, val_data_input_index_list_for_hist, validation_data_list_for_hist, output_list) \n",
    "               \n",
    "            result_history_list.append([train_times_count, sse, mse])\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "       \n",
    "       #     pred_result_list.append(sse)\n",
    "       #     best_result= min(pred_result_list)\n",
    "       #     if len(pred_result_list)> 1 and sse== best_result:\n",
    "       #        save_model(model, save_model_dir)\n",
    "       #        print ('Save the best result :', sse)\n",
    "        #       call_back_stop_count= 0\n",
    "        #    call_back_stop_count+= 1\n",
    "   \n",
    "        print (result_history_list)\n",
    "        train_times_count+= 1\n",
    "        save_train_history_data(save_train_history_dir, result_history_list)  \n",
    "          \n",
    " \n",
    "      \n",
    "    return model, validation_input_data_list, validation_data_input_index_list, input_data_list\n",
    "\n",
    "\n",
    "\n",
    "def on_preprocessing_layer_multi_hot(input_data_list, features):\n",
    "    layer= tf.keras.layers.CategoryEncoding(num_tokens= features, output_mode=\"multi_hot\")\n",
    "    new_input_data_list= layer(input_data_list)\n",
    "    #new_input_data_list= tf.keras.layers.Embedding( # Input_dim: Integer. i.e. maximum integer index + 1.\n",
    "    #  1000,\n",
    "    #  len(input_data_list),\n",
    "    #  embeddings_initializer=\"uniform\",\n",
    "    #  embeddings_regularizer=None,\n",
    "    #  activity_regularizer=None,\n",
    "    #  embeddings_constraint=None,\n",
    "    #  mask_zero=False,\n",
    "    #  input_length=None,\n",
    "    #)(input_data_list)\n",
    "    return new_input_data_list\n",
    "\n",
    "\n",
    "def on_preprocessing_layer(input_data_list, data_size):\n",
    "    #data_range= len(input_data_list)#+ 6\n",
    "    layer= tf.keras.layers.CategoryEncoding(num_tokens= data_size, output_mode=\"one_hot\")\n",
    "    new_input_data_list= layer(input_data_list)\n",
    "    #new_input_data_list= tf.keras.layers.Embedding( # Input_dim: Integer. i.e. maximum integer index + 1.\n",
    "    #  1000,\n",
    "    #  len(input_data_list),\n",
    "    #  embeddings_initializer=\"uniform\",\n",
    "    #  embeddings_regularizer=None,\n",
    "    #  activity_regularizer=None,\n",
    "    #  embeddings_constraint=None,\n",
    "    #  mask_zero=False,\n",
    "    #  input_length= None,\n",
    "    #)(new_input_data_list)\n",
    "  \n",
    "    return new_input_data_list\n",
    "\n",
    "def on_input_layer(input_data_list, batch_size, time_steps, features):\n",
    "   # input= [samples, time_steps, features] [Number of datas, The length of each data, Each element of the data is a vector of n features]\n",
    "   # Samples - Number of datas\n",
    "   # Time steps -   The length of each data\n",
    "   # Features - Each element of the data is a vector of n features\n",
    "    input_layer = tf.keras.Input(shape=(time_steps, features), name='input_layer') # shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "    return input_layer\n",
    "  \n",
    "def on_next_layer(layer, layer_count, num_of_layers, state, time_steps, features):\n",
    "    if state[0]== None:\n",
    "        state= None\n",
    "    if layer_count== num_of_layers-1:\n",
    "        dropout= 0\n",
    "    else:\n",
    "        dropout= 0\n",
    "    number_of_units= 256\n",
    "\n",
    "  # if layer_count %2== 0:\n",
    "  #   number_of_units= 14\n",
    "  #   state= None   \n",
    "  #   new_layer, memory_state, carry_state= tf.keras.layers.LSTM(units = number_of_units, name= 'hidden_layer_'+ str(layer_count), return_sequences=True, stateful= True, return_state= True\n",
    "  #                                 , time_major= True, activation=\"selu\",\n",
    "  #   recurrent_activation=\"selu\", unit_forget_bias=True,\n",
    "  #   use_bias=True)(layer, state)\n",
    "  # else:\n",
    "  #   number_of_units= 14-1\n",
    "  #   new_layer, _, _= tf.keras.layers.LSTM(units = number_of_units, name= 'hidden_layer_'+ str(layer_count), return_sequences=True, stateful= True, return_state= True\n",
    "  #                                 , time_major= True, activation=\"selu\",\n",
    "  #   recurrent_activation=\"selu\", unit_forget_bias=True,\n",
    "  #   use_bias=True)(layer)\n",
    "  #\n",
    "  #   carry_state= None\n",
    "  #   memory_state= None\n",
    "   \n",
    "    lstm= tf.keras.layers.LSTM(units = number_of_units, input_shape= (time_steps, features), name= 'hidden_layer_'+ str(layer_count), return_sequences=True, stateful= False, return_state= True\n",
    "                                  , time_major= False, activation=\"tanh\", recurrent_activation=\"sigmoid\", unit_forget_bias=True, \n",
    "                                  kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", dropout= dropout, \n",
    "                                  use_bias=True)\n",
    "  \n",
    "    new_layer, memory_state, carry_state= lstm(layer, state)\n",
    "    \n",
    "    num_heads= int(math.sqrt(number_of_units))\n",
    " \n",
    "    if layer_count== num_of_layers-1:\n",
    "       \n",
    "       \n",
    "        new_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(features))(new_layer) # TimeDistributed applies the same instance of previous layer to each of the timestamps, the same set of weights are used at each timestamp.#\n",
    "\n",
    "    return new_layer, memory_state, carry_state\n",
    "\n",
    "\n",
    "\n",
    "def on_output_layer(intput_data_list, output_value_list, layer, time_steps, features):\n",
    " \n",
    "    outputs= tf.keras.layers.Flatten(data_format=None)(layer)\n",
    "    outputs= tf.keras.layers.Dropout(0.4) (outputs, training=True)\n",
    "    outputs= tf.keras.layers.Dense(units = len(intput_data_list[0]), activation='relu', name='output_dense_layer_0', use_bias=True)(outputs)\n",
    "    outputs= tf.keras.layers.Dense(units = len(intput_data_list[0]), activation='sigmoid', name='output_dense_layer_1', use_bias=True)(outputs)\n",
    "    outputs= tf.keras.layers.Dense(units = len(output_value_list[0]), activation='relu', name='output_dense_layer_2', use_bias=True)(outputs)\n",
    "    outputs= tf.keras.layers.Dense(units = len(output_value_list[0]), activation='sigmoid', name='output_dense_layer_3', use_bias=True)(outputs)\n",
    "    print ('outputs shape', outputs.shape)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def train_model(input_data_list, data_expected_output, model, checkpoint_dir, batch_size):\n",
    "    optimizer_Adam= tf.keras.optimizers.Adam(learning_rate= 1e-3,\n",
    "                                              beta_1=0.9,\n",
    "                                              beta_2=0.999,\n",
    "                                              epsilon=1e-07,\n",
    "    )\n",
    "\n",
    "    rms_prop= tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=1e-3,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name=\"RMSprop\",\n",
    "    )\n",
    "\n",
    "    ada_delta= tf.keras.optimizers.Adadelta(\n",
    "        learning_rate=1e-3, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    "    )\n",
    "\n",
    "    sgd= tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "    binary_crossentropy= tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits= True,\n",
    "                label_smoothing=0.0,\n",
    "                axis=-1,\n",
    "                reduction=\"auto\",\n",
    "                name=\"binary_crossentropy\",\n",
    "    )\n",
    "\n",
    "    categorical_crossentropy= tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        label_smoothing=0.0,\n",
    "        axis=-1,\n",
    "      #  reduction=\"auto\",\n",
    "        name=\"categorical_crossentropy\",\n",
    "    ) \n",
    "    early_stopping= tf.keras.callbacks.EarlyStopping(\n",
    "            # Stop training when `val_loss` is no longer improving\n",
    "            monitor=\"val_loss\", # val_loss\n",
    "            # \"No longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "            min_delta=1e-2,\n",
    "            # \"No longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            mode= 'auto'\n",
    "    )\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "        early_stopping,\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_dir,\n",
    "            monitor='val_accuracy',\n",
    "            mode='auto',\n",
    "            save_weights_only= True,\n",
    "            save_best_only=True\n",
    "        )    \n",
    "    ]\n",
    " \n",
    "\n",
    "    model.compile(#optimizer= sgd,\n",
    "              optimizer= optimizer_Adam, \n",
    "              # Loss function to minimize\n",
    "              loss= tf.keras.losses.CategoricalCrossentropy(),\n",
    "              # List of metrics to monitor    \n",
    "              metrics=['accuracy', 'mse']\n",
    "            )\n",
    "\n",
    "    print ('input_data_list shape', input_data_list.shape)\n",
    "    print ('data_expected_output shape', data_expected_output.shape)\n",
    "  \n",
    " \n",
    "    print('# Fit model on training data')\n",
    "    model.summary()\n",
    "    batch_size= 32\n",
    "    number_of_epochs= 50\n",
    "\n",
    "    history = model.fit(input_data_list, \n",
    "            data_expected_output, \n",
    "        batch_size= batch_size, \n",
    "        epochs= number_of_epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks= callbacks,\n",
    "                )\n",
    "  \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    " # results = model.evaluate(validation_input_data_list, validation_expected_output, batch_size=128)\n",
    " # print(\"test loss, test acc:\", results)\n",
    "    return history\n",
    "\n",
    "def build_model(batch_size, timesteps, features):\n",
    "    batch_size= 3\n",
    "    units= 512\n",
    "    features= 5\n",
    "    output_size= 3\n",
    "    timesteps= 5\n",
    "    # CuDNN is only available at the layer level, and not at the cell level.\n",
    "    # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "    input_layer = tf.keras.layers.Input(shape=(timesteps, features))\n",
    "        # The LSTM layer with default options uses CuDNN.\n",
    "    lstm_layer= tf.keras.layers.LSTM(units, input_shape=(timesteps, features)\n",
    "                                  , activation=\"tanh\",\n",
    "    recurrent_activation=\"sigmoid\", unit_forget_bias=True, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\",\n",
    "    use_bias=True, batch_size= batch_size)\n",
    "   \n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(Model, dir):\n",
    "    Model.save(\n",
    "        dir,\n",
    "      overwrite=True,\n",
    "      include_optimizer=True,\n",
    "      save_format=None,\n",
    "      signatures=None,\n",
    "      options=None,\n",
    "      save_traces=True,\n",
    "    )\n",
    "\n",
    "def load_model(dir):\n",
    "    loaded_model = tf.keras.models.load_model(dir)\n",
    "    return loaded_model\n",
    "\n",
    "def model_predict(predict_data, model):\n",
    "  \n",
    "    prediction= model.predict(\n",
    "        predict_data,\n",
    "        batch_size=None,\n",
    "        verbose=\"auto\",\n",
    "        steps=None,\n",
    "        callbacks=None,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False,\n",
    "      )\n",
    "    return prediction\n",
    "\n",
    "def init_predict_on_training(model, all_segments, predict_input_data_list, predict_input_data_index, predict_data_list, output_list): \n",
    "\n",
    "    predict_input_data_list_arr= np.array(predict_input_data_list) \n",
    "\n",
    "    expected_output_value_list= get_expected_output_data(predict_input_data_index, predict_data_list, output_list)\n",
    "    output_score_list= [i[0] for i in output_list]\n",
    "  \n",
    "    count= 1\n",
    "    SSE= 0\n",
    "    for predict_data, predict_data_index, expected_output_value_index in zip(predict_input_data_list_arr, predict_input_data_index, expected_output_value_list):\n",
    "        detail= None\n",
    "        SE= 0\n",
    "        time_steps, features= predict_data.shape\n",
    "        new_predict_data= predict_data.reshape(1, time_steps, features)\n",
    "        prediction= model_predict(new_predict_data, model)\n",
    " \n",
    "        max_val= np.max(prediction)\n",
    "        test, value_index= np.where(prediction== max_val)\n",
    "        if len(value_index)!= 1:\n",
    "            print (predict_data_index, \"can't predict data\")\n",
    "            continue\n",
    "        predict_score= output_score_list[int(value_index)]\n",
    "        new_expected_output_value_index= expected_output_value_index.index(1)\n",
    "        expected_output_value= output_score_list[new_expected_output_value_index]\n",
    "  \n",
    "        SE+= (float(predict_score)- float(expected_output_value))** 2\n",
    "        if SE!= 0:\n",
    "            detail= ['predict_score', predict_score, 'expected_score', expected_output_value]\n",
    "        SSE+= SE\n",
    "   #   print (predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail)\n",
    "      \n",
    "   #   if count% 6 == 0:\n",
    "   #     print ()\n",
    "    # print ('predict_score', predict_score )\n",
    "        count+= 1\n",
    "    MSE= SSE/ count\n",
    "    return SSE, MSE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_ans_info_list(input_ans_list, expected_output, output_steps_records_list): \n",
    "    correct_ans_info_list= []\n",
    "    ans_match_list= []\n",
    "    for ans in input_ans_list[0]:\n",
    "        if ans in expected_output:\n",
    "            correct_ans_info= [0, ans[0], ans[1]]\n",
    "            correct_ans_info_list.append(correct_ans_info)\n",
    "        \n",
    "        if not ans in expected_output: \n",
    "            ans_info= [1, ans[0], ans[1]]\n",
    "            ans_match_list.append(ans_info)\n",
    "\n",
    "    step_probability_list= []\n",
    "    for i in output_steps_records_list:\n",
    "        for i_0 in i[2]:\n",
    "            all_search_in_step= True\n",
    "            for c in range(len(ans_match_list)):             \n",
    "                if ans_match_list[c] not in i_0:\n",
    "                    all_search_in_step= False\n",
    "                else:\n",
    "                    if not [i[0], i[1],i_0] in step_probability_list:\n",
    "                        ans_match_list[c].append([i[0], i[1], i_0])\n",
    "                        step_probability_list.append([i[0], i[1], i_0])\n",
    "        \n",
    "            \n",
    "    return ans_match_list, step_probability_list\n",
    "\n",
    "def get_probility_score(full_score, ans_match_list, step_probability_list, input_ans_index_name):\n",
    "    total_score= 0\n",
    "    \n",
    "    input_ans_list_2_upload= [] # Input ans 2 upload\n",
    "\n",
    "    for math_ans_info in ans_match_list:\n",
    "        #print ('math_ans_info', math_ans_info)\n",
    "        match_ans_probability= math_ans_info[3][1]\n",
    "        match_ans_info_list= math_ans_info[3][2]\n",
    "\n",
    "        match_ans_score=  1/ int(match_ans_probability)* full_score\n",
    "        \n",
    "        print (math_ans_info[1], '->', math_ans_info[2], 'wrong answer: get score '+ str(round(match_ans_score, 1)), 'probability '+ str(1)+ '/', int(match_ans_probability))\n",
    "        total_score+= match_ans_score\n",
    "\n",
    "        # Ans info to dataset [src_id, target_id, src_label, target_label, description]\n",
    "        src_id= math_ans_info[1]\n",
    "        target_id= math_ans_info[2]\n",
    "        arrow= 'to' # attribute 'from', 'to\n",
    "        src_label= input_ans_index_name[math_ans_info[1]]\n",
    "        description= 'wrong answer: get score '+ str(round(match_ans_score, 1))\n",
    "        target_label= input_ans_index_name[math_ans_info[2]]\n",
    "        color= 'rgb(255, 0, 0)'\n",
    "        input_ans_list_2_upload.append([src_id, target_id, src_label, target_label, description, color]) \n",
    "\n",
    "    #print ('total_score', total_score)\n",
    "    return total_score, input_ans_list_2_upload\n",
    "\n",
    "def get_output_steps_records_list(dir):\n",
    "    with open(dir, 'r', newline='') as csvfile:\n",
    "        content= []\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            sub_content= []\n",
    "            sub_content.append(row[0])\n",
    "            sub_content.append(int(row[1]))\n",
    "            step_record= ast.literal_eval(row[2])\n",
    "            sub_content.append(step_record)      \n",
    "            content.append(sub_content)\n",
    "      \n",
    "    return content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_rule_data_name= 'train_rule_data_for_con_demo_man.csv'\n",
    "output_steps_records_data_name= 'output_steps_records_list.csv'\n",
    "train_and_val_data_name= 'train_data_for_val.csv'\n",
    "\n",
    "dir= os.getcwd()\n",
    "train_rule_data_dir= os.path.join(dir, train_rule_data_name)\n",
    "save_train_and_val_data_dir= os.path.join(dir, train_and_val_data_name)\n",
    "\n",
    "number_of_segments= 6\n",
    "number_of_outside_number_of_segments= 6\n",
    "\n",
    "outside_segment_data_sort= list(range(101, 100+ number_of_outside_number_of_segments+ 1))\n",
    "expected_data_sort= list(range(1, number_of_segments+ 1))\n",
    "all_segments_data_sort= expected_data_sort+ outside_segment_data_sort\n",
    "\n",
    "number_of_layers= 6\n",
    "\n",
    "output_list= [['-5', 0], ['-4', 0], ['-3', 0], ['-2', 0], ['-1', 0], ['0', 0], ['1', 0], ['2', 0], ['3', 0], ['4', 0], ['5', 0]]\n",
    "#output_list= []\n",
    "#for score in range(-5, 6):\n",
    "#    sub_component= [str(score), 0]\n",
    "#    output_list.append(output_list)\n",
    "\n",
    "gen_new_session_list= True\n",
    "if not os.path.exists(save_train_and_val_data_dir) or gen_new_session_list: \n",
    "    session_list= get_target_train_rule_data_and_validation_data_test_data(train_rule_data_dir) \n",
    "    save_train_and_val_data(save_train_and_val_data_dir, session_list)\n",
    "\n",
    "\n",
    "session_list= get_train_and_val_data(save_train_and_val_data_dir)\n",
    "#session_list= get_cross_train_rule_data_and_validation_data(target_dir)\n",
    "\n",
    "\n",
    "memory_state= None\n",
    "carry_state= None\n",
    "\n",
    "task_name= '_for_conference'#'_for_conference' #_for_conference_with_test_model\n",
    "save_model_dir= os.path.join(dir, 'eva_reward_predict_model_'+ str(number_of_layers)+'_layers'+ task_name) \n",
    "checkpoint_dir= os.path.join(save_model_dir, 'checkpoint/')\n",
    "save_train_history_dir= os.path.join(dir, 'train_history.csv')\n",
    "\n",
    "if not os.path.exists(save_model_dir):\n",
    "    os.mkdir(save_model_dir)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "#output_steps_records_list_target_dir= os.path.join(dir, output_steps_records_data_name)\n",
    "#output_steps_records_list= get_output_steps_records_list(output_steps_records_list_target_dir) # Get output content\n",
    "#model= load_model(save_model_dir)\n",
    "model= False\n",
    "#model, predict_input_data_list, predict_input_data_index, input_data_list= init_training(expected_data_sort, outside_segment_data_sort, session_list, output_list, number_of_layers, model, memory_state, carry_state, save_model_dir, checkpoint_dir, save_train_history_dir, train_rule_data_dir)\n",
    "\n",
    "\n",
    "#weights = model.get_weights() # Getting params\n",
    "#model.set_weights(weights) # Setting params\n",
    "#weights = model.layers[i].get_weights() # Getting params\n",
    "#model.layers[i].set_weights(weights) # Setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgtJPryv6uDY",
    "outputId": "a863af39-6b64-425f-c785-5e49c611c356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"reward_predict_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)       [(None, 28, 28)]     0           []                               \n",
      "                                                                                                  \n",
      " hidden_layer_0 (LSTM)          [(None, 28, 256),    291840      ['input_layer[0][0]']            \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " hidden_layer_1 (LSTM)          [(None, 28, 256),    525312      ['hidden_layer_0[0][0]',         \n",
      "                                 (None, 256),                     'hidden_layer_0[0][1]',         \n",
      "                                 (None, 256)]                     'hidden_layer_0[0][2]']         \n",
      "                                                                                                  \n",
      " hidden_layer_2 (LSTM)          [(None, 28, 256),    525312      ['hidden_layer_1[0][0]',         \n",
      "                                 (None, 256),                     'hidden_layer_1[0][1]',         \n",
      "                                 (None, 256)]                     'hidden_layer_1[0][2]']         \n",
      "                                                                                                  \n",
      " hidden_layer_3 (LSTM)          [(None, 28, 256),    525312      ['hidden_layer_2[0][0]',         \n",
      "                                 (None, 256),                     'hidden_layer_2[0][1]',         \n",
      "                                 (None, 256)]                     'hidden_layer_2[0][2]']         \n",
      "                                                                                                  \n",
      " hidden_layer_4 (LSTM)          [(None, 28, 256),    525312      ['hidden_layer_3[0][0]',         \n",
      "                                 (None, 256),                     'hidden_layer_3[0][1]',         \n",
      "                                 (None, 256)]                     'hidden_layer_3[0][2]']         \n",
      "                                                                                                  \n",
      " hidden_layer_5 (LSTM)          [(None, 28, 256),    525312      ['hidden_layer_4[0][0]',         \n",
      "                                 (None, 256),                     'hidden_layer_4[0][1]',         \n",
      "                                 (None, 256)]                     'hidden_layer_4[0][2]']         \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 28, 28)      7196        ['hidden_layer_5[0][0]']         \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 784)          0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 784)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " output_dense_layer_0 (Dense)   (None, 28)           21980       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " output_dense_layer_1 (Dense)   (None, 28)           812         ['output_dense_layer_0[0][0]']   \n",
      "                                                                                                  \n",
      " output_dense_layer_2 (Dense)   (None, 11)           319         ['output_dense_layer_1[0][0]']   \n",
      "                                                                                                  \n",
      " output_dense_layer_3 (Dense)   (None, 11)           132         ['output_dense_layer_2[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,948,839\n",
      "Trainable params: 2,948,839\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "46 [[[1, 2, 3, 4, 6, 104], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 104], -5, 1], [[1, 2, 3, 4, 6, 106], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 106], -5, 1], [[1, 103, 2, 3, 4, 5], [1, '->', 103], -2, [103, '->', 2], -5, [2, '->', 3], 3, [3, '->', 4], 3, [4, '->', 5], 3, 1], [[1, 2, 104, 3, 4, 5], [1, '->', 2], 5, [2, '->', 104], -5, [104, '->', 3], -5, [3, '->', 4], 3, [4, '->', 5], 3, 1], [[1, 2, 5, 3, 6, 101], [1, '->', 2], 5, [2, '->', 5], -4, [5, '->', 3], -5, [3, '->', 6], -4, [6, '->', 101], -5, 1], [[1, 3, 4, 5, 105, 2], [1, '->', 3], 2, [3, '->', 4], 3, [4, '->', 5], 3, [5, '->', 105], -5, [105, '->', 2], -3, 1], [[1, 5, 2, 3, 101, 4], [1, '->', 5], -4, [5, '->', 2], -5, [2, '->', 3], 3, [3, '->', 101], -5, [101, '->', 4], -5, 1], [[1, 5, 2, 3, 4, 6], [1, '->', 5], -4, [5, '->', 2], -5, [2, '->', 3], 3, [3, '->', 4], 3, [4, '->', 6], 2, 1], [[1, 4, 3, 5, 2, 6], [1, '->', 4], -1, [4, '->', 3], -5, [3, '->', 5], -3, [5, '->', 2], -5, [2, '->', 6], -4, 1], [[2, 1, 3, 4, 6, 101], [2, '->', 1], -2, [1, '->', 3], 1, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 101], -5, 2], [[1, 2, 5, 3, 6, 4], [1, '->', 2], 5, [2, '->', 5], -4, [5, '->', 3], -5, [3, '->', 6], -4, [6, '->', 4], -5, 1], [[2, 1, 106, 3, 4, 5], [2, '->', 1], -2, [1, '->', 106], 1, [106, '->', 3], -2, [3, '->', 4], 3, [4, '->', 5], 3, 2], [[1, 3, 2, 5, 103, 4], [1, '->', 3], 2, [3, '->', 2], -5, [2, '->', 5], -4, [5, '->', 103], -5, [103, '->', 4], -2, 1], [[1, 2, 4, 3, 102, 106], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 3], -5, [3, '->', 102], -5, [102, '->', 106], -5, 1], [[1, 2, 3, 5, 4, 6], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 5], -3, [5, '->', 4], -5, [4, '->', 6], 2, 1], [[1, 2, 6, 3, 103, 105], [1, '->', 2], 5, [2, '->', 6], -4, [6, '->', 3], -5, [3, '->', 103], -5, [103, '->', 105], -5, 1], [[1, 4, 2, 3, 102, 104], [1, '->', 4], -1, [4, '->', 2], -5, [2, '->', 3], 3, [3, '->', 102], -5, [102, '->', 104], -5, 1], [[2, 3, 4, 5, 104, 1], [2, '->', 3], 3, [3, '->', 4], 3, [4, '->', 5], 3, [5, '->', 104], -5, [104, '->', 1], -5, 2], [[1, 2, 4, 3, 6, 105], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 3], -5, [3, '->', 6], -4, [6, '->', 105], -5, 1], [[1, 2, 103, 4, 5, 6], [1, '->', 2], 5, [2, '->', 103], -3, [103, '->', 4], 3, [4, '->', 5], 5, [5, '->', 6], 5, 1], [[1, 2, 3, 4, 5, 6], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 5], 5, [5, '->', 6], 5], [[1, 3, 5, 2, 101, 103], [1, '->', 3], 2, [3, '->', 5], -4, [5, '->', 2], -5, [2, '->', 101], -5, [101, '->', 103], -5, 1], [[1, 2, 3, 102, 4, 5], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 102], -3, [102, '->', 4], -5, [4, '->', 5], 3, 1], [[2, 3, 5, 4, 106, 1], [2, '->', 3], 3, [3, '->', 5], -4, [5, '->', 4], -5, [4, '->', 106], -5, [106, '->', 1], -5, 2], [[1, 2, 6, 3, 103, 106], [1, '->', 2], 5, [2, '->', 6], -4, [6, '->', 3], -5, [3, '->', 103], -5, [103, '->', 106], -5, 1], [[1, 2, 3, 4, 104, 102], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 104], -5, [104, '->', 102], -5, 1], [[1, 2, 4, 3, 104, 102], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 3], -5, [3, '->', 104], -5, [104, '->', 102], -5, 1], [[1, 2, 4, 3, 6, 5], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 3], -5, [3, '->', 6], -4, [6, '->', 5], -5, 1], [[1, 3, 2, 5, 4, 6], [1, '->', 3], 2, [3, '->', 2], -5, [2, '->', 5], -4, [5, '->', 4], -5, [4, '->', 6], 2, 1], [[2, 3, 4, 5, 1, 6], [2, '->', 3], 3, [3, '->', 4], 3, [4, '->', 5], 3, [5, '->', 1], -5, [1, '->', 6], -5, 2], [[1, 2, 5, 3, 6, 103], [1, '->', 2], 5, [2, '->', 5], -4, [5, '->', 3], -5, [3, '->', 6], -4, [6, '->', 103], -5, 1], [[1, 2, 6, 3, 106, 105], [1, '->', 2], 5, [2, '->', 6], -4, [6, '->', 3], -5, [3, '->', 106], -5, [106, '->', 105], -5, 1], [[1, 2, 3, 5, 106, 4], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 5], -3, [5, '->', 106], -5, [106, '->', 4], -5, 1], [[2, 1, 3, 4, 5, 6], [2, '->', 1], -2, [1, '->', 3], 1, [3, '->', 4], 5, [4, '->', 5], 5, [5, '->', 6], 5, 2], [[1, 2, 4, 6, 104, 103], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 6], 1, [6, '->', 104], -5, [104, '->', 103], -5, 1], [[3, 1, 4, 5, 105, 2], [3, '->', 1], -5, [1, '->', 4], -3, [4, '->', 5], 3, [5, '->', 105], -5, [105, '->', 2], -3, 3], [[1, 2, 3, 5, 105, 4], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 5], -3, [5, '->', 105], -5, [105, '->', 4], -5, 1], [[1, 2, 3, 4, 103, 102], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 103], -5, [103, '->', 102], -3, 1], [[1, 102, 3, 4, 6, 2], [1, '->', 102], -5, [102, '->', 3], -3, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 2], -5, 1], [[2, 3, 4, 5, 1, 106], [2, '->', 3], 3, [3, '->', 4], 3, [4, '->', 5], 3, [5, '->', 1], -5, [1, '->', 106], 1, 2], [[1, 3, 4, 5, 2, 6], [1, '->', 3], 2, [3, '->', 4], 3, [4, '->', 5], 3, [5, '->', 2], -5, [2, '->', 6], -4, 1], [[1, 2, 3, 4, 6, 103], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 103], -5, 1], [[1, 2, 3, 5, 101, 104], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 5], -3, [5, '->', 101], -5, [101, '->', 104], -5, 1], [[1, 2, 3, 4, 106, 5], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 106], -5, [106, '->', 5], -5, 1], [[1, 2, 3, 4, 104, 6], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 104], -5, [104, '->', 6], -5, 1], [[1, 2, 3, 4, 6, 105], [1, '->', 2], 5, [2, '->', 3], 5, [3, '->', 4], 5, [4, '->', 6], 1, [6, '->', 105], -5, 1]]\n",
      "5 [[[1, 2, 4, 5, 3, 106], [1, '->', 2], 5, [2, '->', 4], -3, [4, '->', 5], 3, [5, '->', 3], -5, [3, '->', 106], -5, 1], [[104, 1, 3, 4, 5, 6], [104, '->', 1], -2, [1, '->', 3], 1, [3, '->', 4], 5, [4, '->', 5], 5, [5, '->', 6], 5, 104], [[1, 105, 101, 4, 5, 6], [1, '->', 105], -5, [105, '->', 101], -5, [101, '->', 4], -5, [4, '->', 5], 5, [5, '->', 6], 5, 1], [[1, 2, 106, 101, 4, 6], [1, '->', 2], 5, [2, '->', 106], 3, [106, '->', 101], -5, [101, '->', 4], -5, [4, '->', 6], 2, 1], [[1, 2, 102, 4, 5, 6], [1, '->', 2], 5, [2, '->', 102], -5, [102, '->', 4], -5, [4, '->', 5], 5, [5, '->', 6], 5, 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "[[1, 2, 4, 5, 3, 106], [1, 2]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[[1, 2, 4, 5, 3, 106], [2, 4]] (predict score- data score)^ 2, SE:  4.0 detail:  ['predict_score', '-5', 'expected_score', '-3']\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[[1, 2, 4, 5, 3, 106], [4, 5]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '3', 'expected_score', '3']\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[[1, 2, 4, 5, 3, 106], [5, 3]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[1, 2, 4, 5, 3, 106], [3, 106]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[104, 1, 3, 4, 5, 6], [104, 1]] (predict score- data score)^ 2, SE:  4.0 detail:  ['predict_score', '-4', 'expected_score', '-2']\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[104, 1, 3, 4, 5, 6], [1, 3]] (predict score- data score)^ 2, SE:  36.0 detail:  ['predict_score', '-5', 'expected_score', '1']\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[[104, 1, 3, 4, 5, 6], [3, 4]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[104, 1, 3, 4, 5, 6], [4, 5]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[[104, 1, 3, 4, 5, 6], [5, 6]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[[1, 105, 101, 4, 5, 6], [1, 105]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[1, 105, 101, 4, 5, 6], [105, 101]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[1, 105, 101, 4, 5, 6], [101, 4]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[1, 105, 101, 4, 5, 6], [4, 5]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[1, 105, 101, 4, 5, 6], [5, 6]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[1, 2, 106, 101, 4, 6], [1, 2]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "[[1, 2, 106, 101, 4, 6], [2, 106]] (predict score- data score)^ 2, SE:  64.0 detail:  ['predict_score', '-5', 'expected_score', '3']\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "[[1, 2, 106, 101, 4, 6], [106, 101]] (predict score- data score)^ 2, SE:  1.0 detail:  ['predict_score', '-4', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[1, 2, 106, 101, 4, 6], [101, 4]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[1, 2, 106, 101, 4, 6], [4, 6]] (predict score- data score)^ 2, SE:  49.0 detail:  ['predict_score', '-5', 'expected_score', '2']\n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[1, 2, 102, 4, 5, 6], [1, 2]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[[1, 2, 102, 4, 5, 6], [2, 102]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "[[1, 2, 102, 4, 5, 6], [102, 4]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '-5', 'expected_score', '-5']\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "[[1, 2, 102, 4, 5, 6], [4, 5]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "[[1, 2, 102, 4, 5, 6], [5, 6]] (predict score- data score)^ 2, SE:  0.0 detail:  ['Match  ', 'predict_score', '5', 'expected_score', '5']\n",
      "\n",
      "26\n",
      "SSE 158.0\n",
      "MSE 6.076923076923077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "158.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir= os.getcwd()\n",
    "save_model_dir= os.path.join(dir, 'eva_reward_predict_model_'+ str(number_of_layers)+ '_layers'+ task_name)\n",
    "\n",
    "model= load_model(save_model_dir)\n",
    "model.summary()\n",
    "\n",
    "def load_model(dir):\n",
    "    loaded_model = tf.keras.models.load_model(dir)\n",
    "    return loaded_model\n",
    "\n",
    "def model_predict(predict_data, model):\n",
    " \n",
    "  prediction= model.predict(\n",
    "    predict_data,\n",
    "    batch_size=None,\n",
    "    verbose=\"auto\",\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    "  )\n",
    "  return prediction\n",
    "\n",
    "def save_predict_data(target_dir, data):\n",
    "    with open(target_dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "def save_score_list(target_dir, data):\n",
    "    with open(target_dir, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "\n",
    "def init_predict(dir, model, all_segments, predict_input_data_list, predict_input_data_index, predict_data_list, output_list): \n",
    "\n",
    "  \n",
    "    predict_input_data_list_arr= np.array(predict_input_data_list) \n",
    "\n",
    "    expected_output_value_list= get_expected_output_data(predict_input_data_index, predict_data_list, output_list)\n",
    "    output_score_list= [i[0] for i in output_list]\n",
    "\n",
    "    count= 1\n",
    "    SSE= 0\n",
    "    save_predict_data_list= []\n",
    "    total_score= 0\n",
    "    total_score_list= []\n",
    "    for predict_data, predict_data_index, expected_output_value_index in zip(predict_input_data_list_arr, predict_input_data_index, expected_output_value_list):\n",
    "        detail= 'Match'\n",
    "        SE= 0\n",
    "        time_steps, features= predict_data.shape\n",
    "        new_predict_data= predict_data.reshape(1, time_steps, features)\n",
    "        prediction= model_predict(new_predict_data, model)\n",
    " \n",
    "        max_val= np.max(prediction)\n",
    "        test, value_index= np.where(prediction== max_val)\n",
    "        if len(value_index)!= 1:\n",
    "            print (predict_data_index, \"can't predict data\")\n",
    "            continue\n",
    "        predict_score= output_score_list[int(value_index)]\n",
    "        new_expected_output_value_index= expected_output_value_index.index(1)\n",
    "        expected_output_value= output_score_list[new_expected_output_value_index]\n",
    "  \n",
    "        SE+= (float(predict_score)- float(expected_output_value))** 2\n",
    "        if SE!= 0:\n",
    "            detail= ['predict_score', predict_score, 'expected_score', expected_output_value]\n",
    "        else:\n",
    "            detail= ['Match  ', 'predict_score', predict_score, 'expected_score', expected_output_value]\n",
    "        SSE+= SE\n",
    "        print (predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail)\n",
    "        data_row= [predict_data_index, '(predict score- data score)^ 2, SE: ',SE, 'detail: ', detail]\n",
    "        save_predict_data_list.append(data_row)\n",
    "        total_score+= float(predict_score)\n",
    "        if count% (int(len(predict_data_list[0][0]))- 1) == 0:\n",
    "            save_predict_data_list.append([])\n",
    "            total_score_list.append([predict_data_index[0], total_score])\n",
    "            total_score= 0\n",
    "            print ()\n",
    "    # print ('predict_score', predict_score )\n",
    "        count+= 1\n",
    "    print (count)\n",
    "    MSE= SSE/ count\n",
    "    print ('SSE', SSE)\n",
    "    print ('MSE', MSE)\n",
    "\n",
    "    \n",
    "    predict_data_dir= os.path.join(dir, 'predict_data.csv')\n",
    "    predict_score_data_dir= os.path.join(dir, 'predict_score_data.csv')\n",
    "    save_predict_data(predict_data_dir, save_predict_data_list)\n",
    "    save_score_list(predict_score_data_dir, total_score_list)\n",
    "    \n",
    "\n",
    "    return SSE\n",
    "\n",
    "    \n",
    "session_index= 1\n",
    "\n",
    "train_rule_data_list= session_list[session_index][0]\n",
    "validation_data_list= session_list[session_index][1]\n",
    "predict_data_list= validation_data_list\n",
    "\n",
    "input_data_list, input_data_index_list, val_input_data_list, val_data_input_index_list= get_input_data(expected_data_sort, outside_segment_data_sort, train_rule_data_list, validation_data_list)\n",
    "\n",
    "\n",
    "\n",
    "init_predict(dir, model, expected_data_sort, val_input_data_list, val_data_input_index_list, predict_data_list, output_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW3TQh-AUmBI"
   },
   "outputs": [],
   "source": [
    "#number_of_trainin_data= 20\n",
    "#SSE 104.0\n",
    "#MSE 4.0\n",
    "# history_list= [[0, 328.0, 15.619047619047619], [5, 385.0, 18.333333333333332], [10, 789.0, 37.57142857142857], [15, 494.0, 23.523809523809526], [20, 519.0, 24.714285714285715], [25, 617.0, 29.38095238095238], [30, 545.0, 25.952380952380953], [35, 297.0, 14.142857142857142], [40, 801.0, 38.142857142857146], [45, 459.0, 21.857142857142858], [50, 997.0, 47.476190476190474]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7nUDvIq0ZuR",
    "outputId": "230adc07-8b5a-4ccd-8887-8b33c86e9113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "{0: 11, 1: 3, 2: 12, 3: 8, 4: 2, 5: 4, 6: 7, 7: 1, 8: 9, 9: 6, 10: 5, 11: 10}\n",
      "11\n",
      "{0: 9, 1: 3, 2: 10, 3: 12, 4: 1, 5: 11, 6: 5, 7: 2, 8: 7, 9: 4, 10: 8, 11: 6}\n",
      "9\n",
      "{0: 6, 1: 8, 2: 1, 3: 9, 4: 4, 5: 11, 6: 5, 7: 10, 8: 2, 9: 12, 10: 7, 11: 3}\n",
      "6\n",
      "{0: 10, 1: 6, 2: 12, 3: 5, 4: 2, 5: 7, 6: 8, 7: 9, 8: 3, 9: 11, 10: 4, 11: 1}\n",
      "10\n",
      "{0: 4, 1: 11, 2: 9, 3: 10, 4: 3, 5: 8, 6: 2, 7: 12, 8: 6, 9: 1, 10: 5, 11: 7}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "a= list(range(1, 13))\n",
    "print (a)\n",
    "\n",
    "rand_list= []\n",
    "for i in range(5):\n",
    "  c_a= a.copy()\n",
    "  random.shuffle(c_a)\n",
    "  rand_list.append(c_a)\n",
    "\n",
    "for i in rand_list:\n",
    "  rand_2_dict= dict(enumerate(i))\n",
    "\n",
    "  print (rand_2_dict)\n",
    "  print (rand_2_dict[0])\n",
    "\n",
    "dir= '/content/drive/MyDrive/Colab_drive/'\n",
    "save_rand_data_dir= os.path.join(dir, 'rand_data.csv')\n",
    "with open(save_rand_data_dir, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    writer.writerows(rand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0wYhVW34mkX",
    "outputId": "3d19c80a-3847-41e8-e69f-af68cd95df94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '8', '7', '12', '1', '6', '2', '3', '11', '5', '10', '9']\n",
      "{0: '4', 1: '8', 2: '7', 3: '12', 4: '1', 5: '6', 6: '2', 7: '3', 8: '11', 9: '5', 10: '10', 11: '9'}\n",
      "['2', '9', '7', '5', '3', '1', '6', '12', '8', '11', '10', '4']\n",
      "{0: '2', 1: '9', 2: '7', 3: '5', 4: '3', 5: '1', 6: '6', 7: '12', 8: '8', 9: '11', 10: '10', 11: '4'}\n",
      "['12', '8', '11', '3', '9', '4', '5', '10', '2', '6', '7', '1']\n",
      "{0: '12', 1: '8', 2: '11', 3: '3', 4: '9', 5: '4', 6: '5', 7: '10', 8: '2', 9: '6', 10: '7', 11: '1'}\n",
      "['7', '2', '8', '12', '11', '3', '1', '4', '6', '5', '9', '10']\n",
      "{0: '7', 1: '2', 2: '8', 3: '12', 4: '11', 5: '3', 6: '1', 7: '4', 8: '6', 9: '5', 10: '9', 11: '10'}\n",
      "['2', '8', '3', '5', '6', '9', '12', '10', '4', '1', '7', '11']\n",
      "{0: '2', 1: '8', 2: '3', 3: '5', 4: '6', 5: '9', 6: '12', 7: '10', 8: '4', 9: '1', 10: '7', 11: '11'}\n"
     ]
    }
   ],
   "source": [
    "dir= '/content/drive/MyDrive/Colab_drive/'\n",
    "save_rand_data_dir= os.path.join(dir, 'rand_data.csv')\n",
    "with open(save_rand_data_dir, 'r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    rand_list= [i for i in reader]\n",
    "     \n",
    "\n",
    "for i in rand_list:\n",
    "    rand_2_dict= dict(enumerate(i))\n",
    "    print (i)\n",
    "    print (rand_2_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kofjVZbgIe4b"
   },
   "outputs": [],
   "source": [
    "#number_of_trainin_data= 20\n",
    "#SSE 104.0\n",
    "#MSE 4.0\n",
    "#history_list=[[0, 894.0, 24.833333333333332], [5, 930.0, 25.833333333333332], [10, 625.0, 17.36111111111111], [15, 969.0, 26.916666666666668], [20, 881.0, 24.47222222222222], [25, 559.0, 15.527777777777779], [30, 838.0, 23.27777777777778], [35, 1025.0, 28.47222222222222], [40, 926.0, 25.72222222222222]]\n",
    "\n",
    "\n",
    "#number_of_trainin_data= 30\n",
    "#MSE= 7.038\n",
    "#history_list= [[0, 894.0, 24.833333333333332], [5, 930.0, 25.833333333333332], [10, 625.0, 17.36111111111111], [15, 989.0, 27.47222222222222], [20, 616.0, 17.11111111111111], [25, 642.0, 17.833333333333332], [30, 1200.0, 33.333333333333336], [35, 822.0, 22.833333333333332], [40, 654.0, 18.166666666666668]]\n",
    "\n",
    "#number_of_data= 40\n",
    "#hitory_list= [[0, 894.0, 24.833333333333332], [5, 930.0, 25.833333333333332], [10, 625.0, 17.36111111111111], [15, 989.0, 27.47222222222222], [20, 694.0, 19.27777777777778], [25, 593.0, 16.47222222222222], [30, 881.0, 24.47222222222222], [35, 1012.0, 28.11111111111111], [40, 928.0, 25.77777777777778]]\n",
    "\n",
    "# number_of_data= 50\n",
    "#hitory_list= [[0, 894.0, 24.833333333333332], [5, 930.0, 25.833333333333332], [10, 625.0, 17.36111111111111], [15, 989.0, 27.47222222222222], [20, 694.0, 19.27777777777778], [25, 614.0, 17.055555555555557], [30, 913.0, 25.36111111111111], [35, 948.0, 26.333333333333332], [40, 711.0, 19.75]]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
